import re
import testing

import yang.schema
import yang.syntax

class EOF(Exception):
    def __init__(self):
        self.msg = "EOF"
        pass

# Mutable parser state representation
class TokenizerState(object):
    lines: list[str]
    buf: str
    offset: int
    line_num: int
    is_1_1: bool
    strict_quoting: bool
    errors: list[str]
    
    def __init__(self, text: str, line_num: int=0, strict_quoting: bool=True):
        self.lines = text.splitlines(True)
        self.buf = ''
        self.offset = 0
        self.line_num = line_num
        self.is_1_1 = False
        self.strict_quoting = strict_quoting
        self.errors = []

# Pure parsing functions that mutate state in place

def readline(state: TokenizerState) -> None:
    """Read next line from input, mutating state"""
    if len(state.lines) == 0:
        raise EOF()
    state.buf = state.lines.pop(0)
    state.line_num += 1
    state.offset = 0

def set_buf(state: TokenizerState, i: int) -> None:
    """Update buffer position, mutating state"""
    state.offset = state.offset + i
    state.buf = state.buf[i:]

def skip_whitespace_and_comments(state: TokenizerState) -> None:
    """Skip whitespace and comments, mutating state"""
    buflen = len(state.buf)
    
    # Skip whitespace
    while True:
        state.buf = state.buf.lstrip(" \n")
        if state.buf == '':
            readline(state)
            buflen = len(state.buf)
        else:
            state.offset += (buflen - len(state.buf))
            break
    
    # Skip comments
    if len(state.buf) >= 2 and state.buf[0] == '/':
        if state.buf[1] == '/':
            # Line comment - skip to next line
            readline(state)
            skip_whitespace_and_comments(state)
        elif state.buf[1] == '*':
            # Block comment
            i = state.buf.find('*/')
            while i == -1:
                readline(state)
                i = state.buf.find('*/')
            set_buf(state, i + 2)
            skip_whitespace_and_comments(state)

def parse_keyword(state: TokenizerState) -> (str, ?str):
    """Parse a keyword, returning (identifier, prefix)"""
    skip_whitespace_and_comments(state)
    
    m = re.match(yang.syntax.keyword, state.buf)
    if m is not None:
        set_buf(state, m.end_pos)
        # Check separator
        if (state.buf[0].isspace() or
            (len(state.buf) >= 2 and state.buf[0] == '/' and state.buf[1] in ['/', '*']) or
            (state.buf[0] in [';', r'{'])):
            pass
        else:
            raise ValueError(f"expected separator, got: {state.buf[:6]}...")
        
        gident = m.group[3]
        gprefix = m.group[2]
        prefix = None
        if gprefix is not None:
            prefix = gprefix
        if gident is not None:
            return (gident, prefix)
        raise ValueError("identifier or prefix is None")
    else:
        raise ValueError(f"illegal keyword: {state.buf}")

def peek_char(state: TokenizerState) -> str:
    """Peek at next character without consuming"""
    skip_whitespace_and_comments(state)
    if len(state.buf) == 0:
        raise EOF()
    return state.buf[0]

def skip_char(state: TokenizerState) -> None:
    """Skip one character"""
    skip_whitespace_and_comments(state)
    set_buf(state, 1)

def parse_strings(state: TokenizerState, need_quote: bool=False) -> list[(str, str)]:
    """Parse string arguments, returning strings"""
    skip_whitespace_and_comments(state)
    
    if state.buf[0] == ';' or state.buf[0] == r'{' or state.buf[0] == r'}':
        raise ValueError(f"expected argument, got: {state.buf[0]}")
    
    if state.buf[0] == '"' or state.buf[0] == "'":
        # Quoted string
        quote_char = state.buf[0]
        strs = []
        res = []
        indentpos = state.offset
        i = 1
        
        while True:
            buflen = len(state.buf)
            start = i
            while i < buflen:
                if state.buf[i] == quote_char:
                    # End of string
                    res.append(state.buf[start:i])
                    strs.append((''.join(res), quote_char))
                    set_buf(state, i + 1)
                    
                    # Check for concatenation
                    skip_whitespace_and_comments(state)
                    if state.buf[0] == '+':
                        set_buf(state, 1)
                        skip_whitespace_and_comments(state)
                        nstrs = parse_strings(state, need_quote=True)
                        strs.extend(nstrs)
                    return strs
                
                elif (quote_char == '"' and
                      state.buf[i] == '\\' and i < (buflen - 1)):
                    # Handle escape sequences
                    special = None
                    if state.buf[i+1] == 'n':
                        special = '\n'
                    elif state.buf[i+1] == 't':
                        special = '\t'
                    elif state.buf[i+1] == '\"':
                        special = '\"'
                    elif state.buf[i+1] == '\\':
                        special = '\\'
                    elif state.strict_quoting and state.is_1_1:
                        raise ValueError(f"illegal escape: {state.buf[i+1]}")
                    elif state.strict_quoting:
                        raise ValueError(f"illegal escape: {state.buf[i+1]}")
                    
                    if special is not None:
                        res.append(state.buf[start:i])
                        res.append(special)
                        i = i + 1
                        start = i + 1
                i = i + 1
            
            # End of line in string
            if i > 2 and state.buf[i-2] == '\r':
                j = i - 3
            else:
                j = i - 2
            k = j
            while j >= 0 and state.buf[j].isspace():
                j = j - 1
            if j != k:
                s = state.buf[start:j+1] + state.buf[k+1:i]
            else:
                s = state.buf[start:i]
            res.append(s)
            readline(state)
            i = 0
            indent = 0
            if quote_char == '"':
                # Skip indentation
                buflen = len(state.buf)
                while (i < buflen and state.buf[i].isspace() and
                       indent <= indentpos):
                    if state.buf[i] == '\t':
                        indent = indent + 8
                    else:
                        indent = indent + 1
                    i = i + 1
                if indent > indentpos + 1:
                    res.append(' ' * (indent - indentpos - 1))
                elif i == buflen:
                    i = 0
        # Loop continues to next line
        return []  # Should never reach here
    
    elif need_quote == True:
        raise ValueError(f"expected quoted string, got: {state.buf}")
    else:
        # Unquoted string
        buflen = len(state.buf)
        i = 0
        while i < buflen:
            if (state.buf[i].isspace() or state.buf[i] == ';' or
                state.buf[i] == '"' or state.buf[i] == "'" or
                state.buf[i] == r'{' or state.buf[i] == r'}' or
                (i + 1 < buflen and state.buf[i:i+2] in ['//', '/*', '*/'])):
                res_str = state.buf[:i]
                set_buf(state, i)
                return [(res_str, '')]
            i = i + 1
        # End of buffer reached
        res_str = state.buf
        set_buf(state, i)
        return [(res_str, '')]

# Parser functions

def parse_statement(state: TokenizerState, parent: ?yang.schema.Statement) -> yang.schema.Statement:
    """Parse a single statement using mutable state"""
    # Get keyword
    identifier, prefix = parse_keyword(state)
    
    # Check for argument
    tok = peek_char(state)
    
    if tok == r'{' or tok == ';':
        arg = None
    else:
        argstrs = parse_strings(state)
        ass = []
        for a in argstrs:
            ass.append(a.0)
        arg = ''.join(ass)
    
    # Check for YANG 1.1
    if identifier == 'yang-version' and arg == '1.1':
        state.is_1_1 = True
        state.strict_quoting = True
    
    stmt = yang.schema.Statement(identifier, arg, prefix)
    
    # Check for substatements
    tok = peek_char(state)
    
    if tok == r'{':
        skip_char(state)  # skip '{'
        
        while True:
            p = peek_char(state)
            if p == r'}':
                break
            substmt = parse_statement(state, stmt)
            stmt.substatements.append(substmt)
        
        skip_char(state)  # skip '}'
    elif tok == ';':
        skip_char(state)  # skip ';'
    else:
        raise ValueError(f"incomplete statement, expected '{{' or ';', got: {tok}")
    
    return stmt

def parse(text: str, strict_quoting: bool=True) -> yang.schema.Statement:
    """Main entry point for parsing YANG text"""
    state = TokenizerState(text, strict_quoting=strict_quoting)
    try:
        return parse_statement(state, None)
    except EOF:
        raise ValueError("Unexpected end of file")

# Compatibility layer for existing code
class Position(object):
    ref: ?str
    line: int
    
    def __init__(self, ref):
        self.ref = ref
        self.line = 0

class YangTokenizer(object):
    """Compatibility wrapper around functional tokenizer"""
    state: TokenizerState
    pos: Position
    buf: str
    offset: int
    is_1_1: bool
    strict_quoting: bool
    
    def __init__(self, text: str, pos, errors=[], strict_quoting: bool=True):
        self.state = TokenizerState(text, strict_quoting=strict_quoting)
        self.state.errors = errors
        self.pos = pos
        self._sync_properties()
    
    def _sync_properties(self):
        """Sync properties from state"""
        self.buf = self.state.buf
        self.offset = self.state.offset
        self.is_1_1 = self.state.is_1_1
        self.strict_quoting = self.state.strict_quoting
        self.pos.line = self.state.line_num
    
    def skip(self) -> None:
        skip_whitespace_and_comments(self.state)
        self._sync_properties()
    
    def get_keyword(self) -> (identifier: str, prefix: ?str):
        ident, pref = parse_keyword(self.state)
        self._sync_properties()
        return (identifier=ident, prefix=pref)
    
    def get_strings(self, need_quote: bool=False) -> list[(str, str)]:
        strs = parse_strings(self.state, need_quote)
        self._sync_properties()
        return strs
    
    def peek(self) -> str:
        c = peek_char(self.state)
        self._sync_properties()
        return c
    
    def skip_tok(self) -> None:
        skip_char(self.state)
        self._sync_properties()

class YangParser(object):
    """Compatibility wrapper around functional parser"""
    tokenizer: YangTokenizer
    pos: Position
    top: yang.schema.Statement
    last_line: int
    
    def __init__(self, text: str, ref=None, strict_quoting: bool=True):
        self.pos = Position(ref)
        self.last_line = 0
        try:
            self.tokenizer = YangTokenizer(text, self.pos, strict_quoting=strict_quoting)
            self.top = self._parse_statement(None)
        except EOF:
            raise ValueError("Unexpected end of file")
    
    def _parse_statement(self, parent):
        # Use the tokenizer's internal state directly
        stmt = parse_statement(self.tokenizer.state, parent)
        self.tokenizer._sync_properties()
        self.last_line = self.tokenizer.state.line_num
        return stmt


# Unit tests for parser components

def _test_tokenizer_skip_whitespace():
    """Test whitespace skipping"""
    text = "   \n  module"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    tok.skip()
    testing.assertEqual("module", tok.buf, "Should skip whitespace")

def _test_tokenizer_skip_line_comment():
    """Test line comment skipping"""
    text = "// this is a comment\nmodule"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    tok.skip()
    testing.assertEqual("module", tok.buf[:6], "Should skip line comment")

def _test_tokenizer_skip_block_comment():
    """Test block comment skipping"""
    text = "/* block\n   comment */module"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    tok.skip()
    testing.assertEqual("module", tok.buf[:6], "Should skip block comment")

def _test_tokenizer_get_keyword_simple():
    """Test simple keyword parsing"""
    text = "module test"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    kw = tok.get_keyword()
    testing.assertEqual("module", kw.identifier, "Should parse keyword")
    testing.assertEqual(None, kw.prefix, "Should have no prefix")

def _test_tokenizer_get_keyword_with_prefix():
    """Test keyword with prefix"""
    text = "inet:port-number ;"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    kw = tok.get_keyword()
    testing.assertEqual("port-number", kw.identifier, "Should parse identifier")
    testing.assertEqual("inet", kw.prefix, "Should parse prefix")

def _test_tokenizer_get_strings_unquoted():
    """Test unquoted string parsing"""
    text = "1.1;"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    strs = tok.get_strings()
    testing.assertEqual(1, len(strs), "Should get one string")
    testing.assertEqual("1.1", strs[0].0, "Should parse unquoted string")
    testing.assertEqual("", strs[0].1, "Should have empty quote char")

def _test_tokenizer_get_strings_double_quoted():
    """Test double-quoted string parsing"""
    text = '"hello world";'
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    strs = tok.get_strings()
    testing.assertEqual(1, len(strs), "Should get one string")
    testing.assertEqual("hello world", strs[0].0, "Should parse double-quoted string")
    testing.assertEqual('"', strs[0].1, "Should have double quote char")

def _test_tokenizer_get_strings_single_quoted():
    """Test single-quoted string parsing"""
    text = "'hello world';"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    strs = tok.get_strings()
    testing.assertEqual(1, len(strs), "Should get one string")
    testing.assertEqual("hello world", strs[0].0, "Should parse single-quoted string")
    testing.assertEqual("'", strs[0].1, "Should have single quote char")

def _test_tokenizer_get_strings_concatenated():
    """Test string concatenation with + operator"""
    text = '"hello" + "world";'
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    strs = tok.get_strings()
    testing.assertEqual(2, len(strs), "Should get two strings")
    testing.assertEqual("hello", strs[0].0, "First string")
    testing.assertEqual("world", strs[1].0, "Second string")

def _test_tokenizer_peek():
    """Test peek functionality"""
    text = r"  // comment" + "\n  " + r"{ test"
    pos = Position("test")
    tok = YangTokenizer(text, pos)
    c = tok.peek()
    testing.assertEqual(r"{", c, "Should peek past whitespace and comments")
    # peek should not consume
    c2 = tok.peek()
    testing.assertEqual(r"{", c2, "Peek should not consume")

def _test_parser_simple_statement():
    """Test parsing a simple statement"""
    text = 'description "test";'
    stmt = parse(text)
    testing.assertEqual("description", stmt.kw, "Should parse keyword")
    testing.assertEqual("test", stmt.arg, "Should parse argument")
    testing.assertEqual(0, len(stmt.substatements), "Should have no substatements")

def _test_parser_statement_no_arg():
    """Test parsing statement without argument"""
    text = 'input;'
    stmt = parse(text)
    testing.assertEqual("input", stmt.kw, "Should parse keyword")
    testing.assertEqual(None, stmt.arg, "Should have no argument")

def _test_parser_statement_with_substatements():
    """Test parsing statement with substatements"""
    text = r'''container test {
        description "test container";
        leaf foo {
            type string;
        }
    }'''
    stmt = parse(text)
    testing.assertEqual("container", stmt.kw, "Should parse container")
    testing.assertEqual("test", stmt.arg, "Should parse container name")
    testing.assertEqual(2, len(stmt.substatements), "Should have 2 substatements")
    testing.assertEqual("description", stmt.substatements[0].kw, "First should be description")
    testing.assertEqual("leaf", stmt.substatements[1].kw, "Second should be leaf")

def _test_parser_prefixed_statement():
    """Test parsing statement with prefix"""
    text = 'myext:custom-stmt "value";'
    stmt = parse(text)
    testing.assertEqual("custom-stmt", stmt.kw, "Should parse keyword")
    testing.assertEqual("myext", stmt.prefix, "Should parse prefix")
    testing.assertEqual("value", stmt.arg, "Should parse argument")

def _test_parser_empty_container():
    """Test parsing empty container"""
    text = r'container test { }'
    stmt = parse(text)
    testing.assertEqual("container", stmt.kw, "Should parse container")
    testing.assertEqual(0, len(stmt.substatements), "Should have no substatements")

def _test_parser_string_concatenation():
    """Test parsing concatenated strings"""
    text = 'description "This is a " + "concatenated " + "string";'
    stmt = parse(text)
    testing.assertEqual("description", stmt.kw, "Should parse keyword")
    testing.assertEqual("This is a concatenated string", stmt.arg, "Should concatenate strings")

def _test_parser_error_unexpected_eof():
    """Test error handling for unexpected EOF"""
    text = r'container test {'
    try:
        parse(text)
        testing.error("Should have raised ValueError for unexpected EOF")
    except ValueError as e:
        testing.assertIn("end of file", str(e).lower(), "Should mention EOF")